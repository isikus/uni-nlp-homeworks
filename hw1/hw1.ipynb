{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ePbTYobZyg",
        "colab_type": "text"
      },
      "source": [
        "# Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gI4O3BTwFQk",
        "colab_type": "text"
      },
      "source": [
        "Выполнено на мощностях [Google Colab](https://colab.research.google.com/drive/1JDr378iGS0og3KZs1-tPaS2QlWmi45Ue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NifPKP_nHPeH",
        "colab_type": "text"
      },
      "source": [
        "## Подготовить мини-корпус (4-5 текстов или до 10 тысяч токенов) с разметкой ключевых слов. Желательно указать источник корпуса и описать, в каком виде там были представлены ключевые слова."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KadWDarPKyQJ",
        "colab_type": "text"
      },
      "source": [
        "Возьмём стандарт индустрии для задачи определения ключевых слов -- корпус **Hulth (2003)** на английском языке. В удобном предобработанном виде его можно взять [отсюда](https://github.com/boudinfl/hulth-2003-pre)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfQ9zzOALM81",
        "colab_type": "code",
        "outputId": "3baf365a-9289-48cd-8285-f61d0365ef90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/boudinfl/hulth-2003-pre"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'hulth-2003-pre' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuM_ZnTQLafj",
        "colab_type": "text"
      },
      "source": [
        "Вот как выглядит пример документа в корпусе:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeMOIp8OLZ78",
        "colab_type": "code",
        "outputId": "e5d88c67-fda4-49da-9043-5a302810afa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat hulth-2003-pre/train/100.xml"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
            "<?xml-stylesheet href=\"CoreNLP-to-HTML.xsl\" type=\"text/xsl\"?>\r\n",
            "<root>\r\n",
            "  <document>\r\n",
            "    <sentences>\r\n",
            "      <sentence id=\"1\">\r\n",
            "        <tokens>\r\n",
            "          <token id=\"1\">\r\n",
            "            <word>Separate</word>\r\n",
            "            <lemma>separate</lemma>\r\n",
            "            <CharacterOffsetBegin>0</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>8</CharacterOffsetEnd>\r\n",
            "            <POS>JJ</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"2\">\r\n",
            "            <word>accounts</word>\r\n",
            "            <lemma>account</lemma>\r\n",
            "            <CharacterOffsetBegin>9</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>17</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"3\">\r\n",
            "            <word>go</word>\r\n",
            "            <lemma>go</lemma>\r\n",
            "            <CharacterOffsetBegin>18</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>20</CharacterOffsetEnd>\r\n",
            "            <POS>VBP</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"4\">\r\n",
            "            <word>mainstream</word>\r\n",
            "            <lemma>mainstream</lemma>\r\n",
            "            <CharacterOffsetBegin>21</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>31</CharacterOffsetEnd>\r\n",
            "            <POS>JJ</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"5\">\r\n",
            "            <word>-LSB-</word>\r\n",
            "            <lemma>-lsb-</lemma>\r\n",
            "            <CharacterOffsetBegin>32</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>33</CharacterOffsetEnd>\r\n",
            "            <POS>-LRB-</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"6\">\r\n",
            "            <word>investment</word>\r\n",
            "            <lemma>investment</lemma>\r\n",
            "            <CharacterOffsetBegin>33</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>43</CharacterOffsetEnd>\r\n",
            "            <POS>NN</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"7\">\r\n",
            "            <word>-RSB-</word>\r\n",
            "            <lemma>-rsb-</lemma>\r\n",
            "            <CharacterOffsetBegin>43</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>44</CharacterOffsetEnd>\r\n",
            "            <POS>-RRB-</POS>\r\n",
            "          </token>\r\n",
            "        </tokens>\r\n",
            "      </sentence>\r\n",
            "      <sentence id=\"2\">\r\n",
            "        <tokens>\r\n",
            "          <token id=\"1\">\r\n",
            "            <word>New</word>\r\n",
            "            <lemma>New</lemma>\r\n",
            "            <CharacterOffsetBegin>45</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>48</CharacterOffsetEnd>\r\n",
            "            <POS>NNP</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"2\">\r\n",
            "            <word>entrants</word>\r\n",
            "            <lemma>entrant</lemma>\r\n",
            "            <CharacterOffsetBegin>49</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>57</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"3\">\r\n",
            "            <word>are</word>\r\n",
            "            <lemma>be</lemma>\r\n",
            "            <CharacterOffsetBegin>58</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>61</CharacterOffsetEnd>\r\n",
            "            <POS>VBP</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"4\">\r\n",
            "            <word>shaking</word>\r\n",
            "            <lemma>shake</lemma>\r\n",
            "            <CharacterOffsetBegin>62</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>69</CharacterOffsetEnd>\r\n",
            "            <POS>VBG</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"5\">\r\n",
            "            <word>up</word>\r\n",
            "            <lemma>up</lemma>\r\n",
            "            <CharacterOffsetBegin>70</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>72</CharacterOffsetEnd>\r\n",
            "            <POS>RP</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"6\">\r\n",
            "            <word>the</word>\r\n",
            "            <lemma>the</lemma>\r\n",
            "            <CharacterOffsetBegin>73</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>76</CharacterOffsetEnd>\r\n",
            "            <POS>DT</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"7\">\r\n",
            "            <word>separate-account</word>\r\n",
            "            <lemma>separate-account</lemma>\r\n",
            "            <CharacterOffsetBegin>77</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>93</CharacterOffsetEnd>\r\n",
            "            <POS>JJ</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"8\">\r\n",
            "            <word>industry</word>\r\n",
            "            <lemma>industry</lemma>\r\n",
            "            <CharacterOffsetBegin>94</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>102</CharacterOffsetEnd>\r\n",
            "            <POS>NN</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"9\">\r\n",
            "            <word>by</word>\r\n",
            "            <lemma>by</lemma>\r\n",
            "            <CharacterOffsetBegin>103</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>105</CharacterOffsetEnd>\r\n",
            "            <POS>IN</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"10\">\r\n",
            "            <word>supplying</word>\r\n",
            "            <lemma>supply</lemma>\r\n",
            "            <CharacterOffsetBegin>106</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>115</CharacterOffsetEnd>\r\n",
            "            <POS>VBG</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"11\">\r\n",
            "            <word>Web-based</word>\r\n",
            "            <lemma>web-based</lemma>\r\n",
            "            <CharacterOffsetBegin>116</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>125</CharacterOffsetEnd>\r\n",
            "            <POS>JJ</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"12\">\r\n",
            "            <word>platforms</word>\r\n",
            "            <lemma>platform</lemma>\r\n",
            "            <CharacterOffsetBegin>126</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>135</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"13\">\r\n",
            "            <word>that</word>\r\n",
            "            <lemma>that</lemma>\r\n",
            "            <CharacterOffsetBegin>136</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>140</CharacterOffsetEnd>\r\n",
            "            <POS>WDT</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"14\">\r\n",
            "            <word>give</word>\r\n",
            "            <lemma>give</lemma>\r\n",
            "            <CharacterOffsetBegin>141</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>145</CharacterOffsetEnd>\r\n",
            "            <POS>VBP</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"15\">\r\n",
            "            <word>advisers</word>\r\n",
            "            <lemma>adviser</lemma>\r\n",
            "            <CharacterOffsetBegin>146</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>154</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"16\">\r\n",
            "            <word>the</word>\r\n",
            "            <lemma>the</lemma>\r\n",
            "            <CharacterOffsetBegin>155</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>158</CharacterOffsetEnd>\r\n",
            "            <POS>DT</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"17\">\r\n",
            "            <word>tools</word>\r\n",
            "            <lemma>tool</lemma>\r\n",
            "            <CharacterOffsetBegin>159</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>164</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"18\">\r\n",
            "            <word>to</word>\r\n",
            "            <lemma>to</lemma>\r\n",
            "            <CharacterOffsetBegin>165</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>167</CharacterOffsetEnd>\r\n",
            "            <POS>TO</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"19\">\r\n",
            "            <word>pick</word>\r\n",
            "            <lemma>pick</lemma>\r\n",
            "            <CharacterOffsetBegin>168</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>172</CharacterOffsetEnd>\r\n",
            "            <POS>VB</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"20\">\r\n",
            "            <word>independent</word>\r\n",
            "            <lemma>independent</lemma>\r\n",
            "            <CharacterOffsetBegin>173</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>184</CharacterOffsetEnd>\r\n",
            "            <POS>JJ</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"21\">\r\n",
            "            <word>money</word>\r\n",
            "            <lemma>money</lemma>\r\n",
            "            <CharacterOffsetBegin>185</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>190</CharacterOffsetEnd>\r\n",
            "            <POS>NN</POS>\r\n",
            "          </token>\r\n",
            "          <token id=\"22\">\r\n",
            "            <word>managers</word>\r\n",
            "            <lemma>manager</lemma>\r\n",
            "            <CharacterOffsetBegin>191</CharacterOffsetBegin>\r\n",
            "            <CharacterOffsetEnd>199</CharacterOffsetEnd>\r\n",
            "            <POS>NNS</POS>\r\n",
            "          </token>\r\n",
            "        </tokens>\r\n",
            "      </sentence>\r\n",
            "    </sentences>\r\n",
            "  </document>\r\n",
            "</root>\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho3aqReILsTm",
        "colab_type": "text"
      },
      "source": [
        "Напишем функцию для приведения такого документа к удобному нам формату:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr3XHIQrLx12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHRgGtRuLsGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getHulthDoc(filepath):\n",
        "  with open(filepath, \"r\", encoding=\"utf-8\") as inh:\n",
        "    xmlstr = inh.read()\n",
        "  soup = BeautifulSoup(xmlstr)\n",
        "  doc = [[str(l.text).lower() for l in sentence.findAll(\"lemma\")]\n",
        "         for sentence in soup.root.document.sentences.findAll(\"sentence\")]\n",
        "  return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxRCBzlbLPpD",
        "colab_type": "code",
        "outputId": "c868ab52-5eb9-4d4d-bf0e-3b695808498c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(getHulthDoc(\"hulth-2003-pre/train/100.xml\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['separate', 'account', 'go', 'mainstream', '-lsb-', 'investment', '-rsb-'], ['new', 'entrant', 'be', 'shake', 'up', 'the', 'separate-account', 'industry', 'by', 'supply', 'web-based', 'platform', 'that', 'give', 'adviser', 'the', 'tool', 'to', 'pick', 'independent', 'money', 'manager']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoLs7Q4HUmZ",
        "colab_type": "text"
      },
      "source": [
        "## Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL2tm5zrNURN",
        "colab_type": "text"
      },
      "source": [
        "Выберем подкорпус так, чтобы в нём было чуть менее 10000 токенов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Ig0Mz-Ol02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "from itertools import chain\n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN3WDQ2WOZ0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getCorpusLen(corpus):\n",
        "  return len(tuple(chain.from_iterable(chain.from_iterable(corpus))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCQnOYQiNcLG",
        "colab_type": "code",
        "outputId": "4d2fdcd9-ff0b-49ea-e336-f440b4ae9ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "source_files = sorted(list(os.listdir(\"hulth-2003-pre/train\")))\n",
        "random.shuffle(source_files)\n",
        "selected = []\n",
        "Corpus = []\n",
        "max_len = 10000\n",
        "i = 0\n",
        "\n",
        "while getCorpusLen(Corpus) < max_len:\n",
        "  fname = source_files[i]\n",
        "  if fname in selected:\n",
        "    continue\n",
        "  Corpus.append(getHulthDoc(\"hulth-2003-pre/train/\"+fname))\n",
        "  selected.append(fname)\n",
        "  print(str(i+1)+\".\\t\"+fname)\n",
        "  i += 1\n",
        "\n",
        "Corpus = Corpus[:-1]\n",
        "selected = selected[:-1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.\t797.xml\n",
            "2.\t544.xml\n",
            "3.\t903.xml\n",
            "4.\t928.xml\n",
            "5.\t1031.xml\n",
            "6.\t1445.xml\n",
            "7.\t108.xml\n",
            "8.\t771.xml\n",
            "9.\t1323.xml\n",
            "10.\t559.xml\n",
            "11.\t1169.xml\n",
            "12.\t826.xml\n",
            "13.\t1382.xml\n",
            "14.\t641.xml\n",
            "15.\t709.xml\n",
            "16.\t539.xml\n",
            "17.\t780.xml\n",
            "18.\t1047.xml\n",
            "19.\t755.xml\n",
            "20.\t1305.xml\n",
            "21.\t1415.xml\n",
            "22.\t809.xml\n",
            "23.\t1318.xml\n",
            "24.\t1430.xml\n",
            "25.\t648.xml\n",
            "26.\t1439.xml\n",
            "27.\t640.xml\n",
            "28.\t587.xml\n",
            "29.\t1334.xml\n",
            "30.\t791.xml\n",
            "31.\t655.xml\n",
            "32.\t944.xml\n",
            "33.\t815.xml\n",
            "34.\t1458.xml\n",
            "35.\t93.xml\n",
            "36.\t1338.xml\n",
            "37.\t624.xml\n",
            "38.\t744.xml\n",
            "39.\t1050.xml\n",
            "40.\t1391.xml\n",
            "41.\t1053.xml\n",
            "42.\t1240.xml\n",
            "43.\t793.xml\n",
            "44.\t117.xml\n",
            "45.\t143.xml\n",
            "46.\t808.xml\n",
            "47.\t116.xml\n",
            "48.\t1138.xml\n",
            "49.\t763.xml\n",
            "50.\t1104.xml\n",
            "51.\t1393.xml\n",
            "52.\t968.xml\n",
            "53.\t1017.xml\n",
            "54.\t1173.xml\n",
            "55.\t1379.xml\n",
            "56.\t618.xml\n",
            "57.\t695.xml\n",
            "58.\t783.xml\n",
            "59.\t1201.xml\n",
            "60.\t615.xml\n",
            "61.\t610.xml\n",
            "62.\t1272.xml\n",
            "63.\t663.xml\n",
            "64.\t1353.xml\n",
            "65.\t1333.xml\n",
            "66.\t89.xml\n",
            "67.\t1303.xml\n",
            "68.\t1015.xml\n",
            "69.\t1185.xml\n",
            "70.\t1000.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JgvKlNXs-h6",
        "colab_type": "text"
      },
      "source": [
        "Ого, здесь около 70 файлов. Многовато! Поэтому доверимся корпусной разметке и пожертвуем уж двумя баллами за этот пункт. Самое время загрузить golden dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6GHpHJ85xjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wE8viqQ5ugl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"hulth-2003-pre/references/train.uncontr.json\", \"r\") as injson:\n",
        "  Data = json.load(injson)\n",
        "\n",
        "GoldenDataset = [list(chain.from_iterable([token.split(\" \") for token in list(chain.from_iterable(Data[fname[:-4]]))])) for fname in selected]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0bdzNCRQzXC",
        "colab_type": "text"
      },
      "source": [
        "Всё же немного изменим образец разметки: подавляющее количесво ключевых слов в нём -- n-граммы. Наши же методы не имеют такой большой склонности к ним, а tf-idf вообще умеет выделять только единичные токены, так что разделим токены дополнительно по пробелу. Так как мы учитываем повторение слов при нашем подсчёте, то потеряем точность мы только на вхождениях с неверным порядком слов (что редко) и вхождениях из одного слова n-граммы (и не факт, что это хуже)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXoQ01pVM0rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "e66c32ab-7222-4122-d71d-ed5d972efc21"
      },
      "source": [
        "GoldenDataset[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adaptive',\n",
              " 'wavelet',\n",
              " 'methods',\n",
              " 'elliptic',\n",
              " 'case',\n",
              " 'operator',\n",
              " 'equations',\n",
              " 'least',\n",
              " 'squares',\n",
              " 'formulation',\n",
              " 'euclidean',\n",
              " 'metric',\n",
              " 'asymptotically',\n",
              " 'optimal',\n",
              " 'complexity',\n",
              " 'n-term',\n",
              " 'approximation']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xxdaF1VHYdP",
        "colab_type": "text"
      },
      "source": [
        "## Применить к этому корпусу 3 метода извлечения ключевых слов на выбор (RAKE, TextRank, tf*idf, OKAPI BM25)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOOsNW6wcjD2",
        "colab_type": "text"
      },
      "source": [
        "Поставим таггеры в равные условия, предоставив им одинаковый набор стоп-слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqhsa-8ecnEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awf54LS1cq7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3c0c55fb-538a-4dfd-b1ea-9ea3468f9c1b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NznGbBcicq1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWWso7OYcuvl",
        "colab_type": "text"
      },
      "source": [
        "И посчитаем всё!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzm7yquiUTSM",
        "colab_type": "text"
      },
      "source": [
        "### RAKE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3TQ409KSsNq",
        "colab_type": "code",
        "outputId": "4c9aa6a0-5184-40c6-f40c-0bd94ec4864b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install python-rake"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-rake in /usr/local/lib/python3.6/dist-packages (1.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RQNvfRXSuNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import RAKE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3u5ryq0S4gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rake = RAKE.Rake(stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VueUmPYeTEVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processRake(doc):\n",
        "  res = rake.run(\" \".join([\" \".join(sent) for sent in doc]), maxWords=3, minFrequency=2)\n",
        "  return [pair[0] for pair in res]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeNcpkKWT99o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RAKE_kw = [processRake(doc) for doc in Corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSKKI1IaYu6O",
        "colab_type": "text"
      },
      "source": [
        "Так как RAKE умеет выдавать токены из нескольких слов, а мы решили работать с однословными, такие вхождения придётся разделить:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxl2mfEdZdcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RAKE_kw = [list(chain.from_iterable([el.split(\" \") for el in token])) for token in RAKE_kw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9e9jKH3UWhC",
        "colab_type": "text"
      },
      "source": [
        "### TextRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75ClQS2UHNh",
        "colab_type": "code",
        "outputId": "4a78af0a-fc0b-4364-c2aa-9292575fac80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pip install summa"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: summa in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from summa) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.19->summa) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdICg9aDUk5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from summa import keywords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qgTrWzvUoSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processTextRank(doc):\n",
        "  res = keywords.keywords(\" \".join([\" \".join(sent) for sent in doc]), additional_stopwords=stop, scores=True)\n",
        "  return [pair[0] for pair in res]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Sz6mzIU43_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TextRank_kw = [processTextRank(doc) for doc in Corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3UBE_RrZony",
        "colab_type": "text"
      },
      "source": [
        "TextRank тоже умеет в многословные токены - исправляем:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEBSRHJmYnsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TextRank_kw = [list(chain.from_iterable([el.split(\" \") for el in token])) for token in TextRank_kw]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQbg4y9DVGJO",
        "colab_type": "text"
      },
      "source": [
        "### tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drC6jQZeXWNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e46OCa0dVIo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stop)\n",
        "transformed = vectorizer.fit_transform([\" \".join([\" \".join(sent) for sent in doc]) for doc in Corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY8kJuhcYsIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        " \n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    sorted_items = sorted_items[:topn]\n",
        " \n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    results = {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlnk9cVuXqEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topn = 5\n",
        "\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "tfidf_kw = []\n",
        "\n",
        "for tf_idf_vector in transformed:\n",
        "  sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
        "  keywords = extract_topn_from_vector(feature_names,sorted_items,topn)\n",
        "  tfidf_kw.append(list(keywords.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bJQPvUCHazP",
        "colab_type": "text"
      },
      "source": [
        "## Оценить точность, полноту, F-меру выбранных методов относительно имеющейся разметки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHveK1WJNAMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cys6g1f9c_sF",
        "colab_type": "text"
      },
      "source": [
        "Сделаем предобработку для списков, чтобы можно было использовать функции для подсчёта метрик качества из библиотеки sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSZOPfqsIgbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(true, pred):\n",
        "  true = deepcopy(true)\n",
        "  pred = deepcopy(pred)\n",
        "  total = true + pred\n",
        "  y_true = [0] * len(total)\n",
        "  y_pred = [0] * len(total)\n",
        "  for i, el in enumerate(total):\n",
        "    for j, tel in enumerate(true):\n",
        "      if tel == el:\n",
        "        true[j] = None\n",
        "        y_true[i] = 1\n",
        "        break\n",
        "    for j, pel in enumerate(pred):\n",
        "      if pel == el:\n",
        "        pred[j] = None\n",
        "        y_pred[i] = 1\n",
        "        break\n",
        "  del_idx = []\n",
        "  for i, _ in enumerate(total):\n",
        "    if not y_true[i] and not y_pred[i]:\n",
        "      del_idx.append(i)\n",
        "  for i in reversed(sorted(del_idx)):\n",
        "    del y_true[i]\n",
        "    del y_pred[i]\n",
        "  return y_true, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgHJ2p1bbhdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82c025a0-7c0c-48da-d30d-750679b79f13"
      },
      "source": [
        "preprocess([\"a\", \"b\", \"d\", \"a\"], [\"a\", \"c\", \"e\", \"d\", \"a\"])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 1, 1, 1, 0, 0], [1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU7Ul9SzSTn9",
        "colab_type": "text"
      },
      "source": [
        "Сделаем красиво: посчитаем средние precision, recall и F1 по корпусу для всех трёх методов выделения ключевых слов и представим их в процентном виде (умножим на 100 и округлим до двух знаков после запятой)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O61-3nznIctL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_single(true, pred):\n",
        "  y_true, y_pred = preprocess(true, pred)\n",
        "  return precision_score(y_true, y_pred)\n",
        "\n",
        "def recall_single(true, pred):\n",
        "  y_true, y_pred = preprocess(true, pred)\n",
        "  return recall_score(y_true, y_pred)\n",
        "\n",
        "def f1_single(true, pred):\n",
        "  y_true, y_pred = preprocess(true, pred)\n",
        "  return f1_score(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB7qffMFWQTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision(list_of_trues, list_of_preds):\n",
        "  tr = list_of_trues\n",
        "  pr = list_of_preds\n",
        "  score = np.mean([precision_single(tr[i], pr[i]) for i in range(len(tr))])\n",
        "  score = round(score * 100, 2)\n",
        "  return score\n",
        "\n",
        "def recall(list_of_trues, list_of_preds):\n",
        "  tr = list_of_trues\n",
        "  pr = list_of_preds\n",
        "  score = np.mean([recall_single(tr[i], pr[i]) for i in range(len(tr))])\n",
        "  score = round(score * 100, 2)\n",
        "  return score\n",
        "\n",
        "def f1(list_of_trues, list_of_preds):\n",
        "  tr = list_of_trues\n",
        "  pr = list_of_preds\n",
        "  score = np.mean([f1_single(tr[i], pr[i]) for i in range(len(tr))])\n",
        "  score = round(score * 100, 2)\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipMHfq47dNO0",
        "colab_type": "text"
      },
      "source": [
        "Наконец, подсчитаем всё и сведём в табличку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0wWHcYN6mzJ",
        "colab_type": "code",
        "outputId": "6056cac2-6930-4a5a-a0f1-eefb327cd314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "Result = [\n",
        "          [\"RAKE\", precision(GoldenDataset, RAKE_kw), recall(GoldenDataset, RAKE_kw), f1(GoldenDataset, RAKE_kw)],\n",
        "          [\"TextRank\", precision(GoldenDataset, TextRank_kw), recall(GoldenDataset, TextRank_kw), f1(GoldenDataset, TextRank_kw)],\n",
        "          [\"tf-idf\", precision(GoldenDataset, tfidf_kw), recall(GoldenDataset, tfidf_kw), f1(GoldenDataset, tfidf_kw)]\n",
        "]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWY6tbOWZFvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8dd13668-4733-4e06-d701-ceb610a6498a"
      },
      "source": [
        "df = pd.DataFrame.from_records(Result)\n",
        "df.columns = [\"Method\", \"Precision\", \"Recall\", \"F1 score\"]\n",
        "\n",
        "df"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RAKE</td>\n",
              "      <td>33.37</td>\n",
              "      <td>7.70</td>\n",
              "      <td>11.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TextRank</td>\n",
              "      <td>48.23</td>\n",
              "      <td>17.33</td>\n",
              "      <td>24.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tf-idf</td>\n",
              "      <td>60.00</td>\n",
              "      <td>16.35</td>\n",
              "      <td>23.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Method  Precision  Recall  F1 score\n",
              "0      RAKE      33.37    7.70     11.35\n",
              "1  TextRank      48.23   17.33     24.43\n",
              "2    tf-idf      60.00   16.35     23.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNUFhjXqHeGh",
        "colab_type": "text"
      },
      "source": [
        "## Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем."
      ]
    }
  ]
}